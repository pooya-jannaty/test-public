{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxwell’s Equations\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla \\cdot \\vec{E} &= \\frac{\\rho}{\\varepsilon_0} \\\\\\\\\n",
    "\\nabla \\cdot \\vec{B} &= 0 \\\\\\\\\n",
    "\\nabla \\times \\vec{E} &= -\\frac{\\partial \\vec{B}}{\\partial t} \\\\\\\\\n",
    "\\nabla \\times \\vec{B} &= \\mu_0 \\vec{J} + \\mu_0\\varepsilon_0 \\frac{\\partial \\vec{E}}{\\partial t}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary><strong>Here's the title of the expandable</strong></summary>\n",
    "\n",
    "Here is some hidden content — could be math, code explanations, or anything else.\n",
    "\n",
    "You can even include LaTeX math:\n",
    "\n",
    "$$\n",
    "E = mc^2\n",
    "$$\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "details { \n",
    "    summary {\n",
    "        font-size: 18px; \n",
    "        color: blue\n",
    "    }\n",
    "    margin-left: 10px;\n",
    "}\n",
    "details {\n",
    "    summary.h3 {\n",
    "        font-size: 16px; \n",
    "        color: blue;\n",
    "    }\n",
    "    margin-left: 20px;\n",
    "}\n",
    "</style>\n",
    "\n",
    "# Transfer1 Code Walkthrough\n",
    "\n",
    "## Config System\n",
    "\n",
    "<details><summary>Concept: Lazy calls</summary>\n",
    "\n",
    "Using OmegaConf’s DictConfig for lazy calling in PyTorch provides a clean, modular, and dynamic way to define and instantiate models, optimizers and datasets.\n",
    "\n",
    "* You define **what** to instantiate in your config.\n",
    "* You **don’t instantiate** it immediately.\n",
    "* Later, at runtime, you call a function like hydra.utils.instantiate(cfg) to create the actual object.\n",
    "\n",
    "This gives you separation of **declaration** (in config) vs. *execution* (in code)\n",
    "\n",
    "`LazyDict` is actually an alias for `DictConfig` from the omegaconf library. Lazy calls, especially as used in conjunction with `DictConfig` and Hydra/OmegaConf, provide a powerful way to delay instantiation of objects (like models, optimizers, datasets) until the actual runtime, rather than when the config is parsed. Here's an example:\n",
    "\n",
    "YAML config:\n",
    "```yaml\n",
    "model:\n",
    "  _target_: mymodule.models.ResNet\n",
    "  depth: 50\n",
    "```\n",
    "\n",
    "Python code:\n",
    "```python\n",
    "from hydra.utils import instantiate\n",
    "model = instantiate(cfg.model)\n",
    "```\n",
    "No need to import or instantiate the model inside the config—just describe it.\n",
    "</details>\n",
    "\n",
    "<details><summary>Primer: Hydra</summary>\n",
    "\n",
    "We use [Hydra](https://hydra.cc/docs/intro/) for advanced configuration composition and overriding. Here's a primer:\n",
    "\n",
    "- Hydra lets you categorize your configs by `group`s. \n",
    "  - Each option in a group is identified by its `name`. \n",
    "  - Much like radio buttons, you can only select/activate one `name` in each `group`. \n",
    "- `node` specifies the config class type for each `name`. \n",
    "- Once you select one `name` in each `group`, Hydra will copy over the contents of the selected configs and put them under the path indicated by `package`. \n",
    "\n",
    "Here's an example to concretize the concepts of `group`, `name`, `node` and `package`. Imagine you have these dataclasses:\n",
    "\n",
    "```python\n",
    "@attr.define\n",
    "class UnetConfig:\n",
    "    channels: int = 64\n",
    "    num_blocks: int = 4\n",
    "\n",
    "@attr.define\n",
    "class ViTConfig:\n",
    "    hidden_dim: int = 512\n",
    "    depth: int = 6\n",
    "```\n",
    "\n",
    "This is how you register them into Hydra’s config system:\n",
    "\n",
    "```python\n",
    "from hydra.core.config_store import ConfigStore\n",
    "\n",
    "cs = ConfigStore.instance()\n",
    "\n",
    "# Register Unet under the 'net' group.\n",
    "cs.store(\n",
    "    group=\"net\",\n",
    "    name=\"unet\",\n",
    "    node=UnetConfig,\n",
    "    package=\"model\"\n",
    ")\n",
    "\n",
    "# Register ViT under the same group.\n",
    "cs.store(\n",
    "    group=\"net\",\n",
    "    name=\"vit\",\n",
    "    node=ViTConfig,\n",
    "    package=\"model.net\" # Paths can be nested.\n",
    ")\n",
    "```\n",
    "\n",
    "Now if you select `unet` as the default config, like so\n",
    "```yaml\n",
    "defaults:\n",
    "    net: unet\n",
    "```\n",
    "\n",
    "Hydra will put the contents of `UnetConfig` under the path `model` and compose the final configuration as follows.\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "    channels: 64\n",
    "    num_blocks: 4\n",
    "```\n",
    "\n",
    "Conversely, if you select `vit`, the final config that Hydra will compose will look like:\n",
    "```yaml\n",
    "model:\n",
    "    net:\n",
    "        hidden_dim: 512\n",
    "        depth: 6\n",
    "```\n",
    "\n",
    "<details><summary class=h3>What is the `_self_` field anyway?</summary>\n",
    "\n",
    "`_self_` will insert the current file’s fields (outside defaults) at its position, and since later fields override earlier ones, we can use it to control the override precedence. Consider the example\n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "    net: unet\n",
    "    _self_\n",
    "    model.num_blocks: 8\n",
    "```\n",
    "\n",
    "This will result in the config\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "    channels: 64\n",
    "    num_blocks: 8\n",
    "```\n",
    "\n",
    "whereas \n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "    _self_\n",
    "    net: unet\n",
    "    model.num_blocks: 8\n",
    "```\n",
    "\n",
    "would have not changed the default, as it would be equivalent to\n",
    "\n",
    "```yaml\n",
    "model:\n",
    "    num_blocks: 8\n",
    "    channels: 64\n",
    "    num_blocks: 4\n",
    "```\n",
    "\n",
    "but because later variables override earlier ones, `num_blocks: 8` would not have taken effect.\n",
    "\n",
    "</details>\n",
    "\n",
    "</details>\n",
    "\n",
    "<details><summary>Concept: Hydra + attrs</summary>\n",
    "\n",
    "Hydra commonly pais with YAML config files. In this codebase however, instead of pairing Hydra with YAML, we opted for pairing it with a Python-based structured config class using `attrs`. \n",
    "\n",
    "So for example, instead of\n",
    "\n",
    "```yaml\n",
    "defaults:\n",
    "    _self_\n",
    "    data_train: null\n",
    "    data_val: null\n",
    "    ...\n",
    "```\n",
    "\n",
    "we do\n",
    "\n",
    "```python\n",
    "@attrs.define(slots=False)\n",
    "class Config(config.Config):\n",
    "    defaults: List[Any] = attrs.field(\n",
    "        factory=lambda: [\n",
    "            \"_self_\",\n",
    "            {\"data_train\": None},\n",
    "            {\"data_val\": None},\n",
    "            ...\n",
    "        ]\n",
    ")\n",
    "```\n",
    "\n",
    "- The pattern `attrs.field(factory=lambda: ...)` is a common way for indicating the default value for mutable fields, e.g. lists. \n",
    "- Had we done somthing like `attrs.field([\"_self_\", ...])`, we would run into trouble when instantiating more than one `Config` object as now the list would be shared between all objects.\n",
    "</details>\n",
    "\n",
    "<details><summary>Cosmos Training Config</summary>\n",
    "\n",
    "Cosmos configs are registered at `register_configs()`.\n",
    "\n",
    "`cosmos_transfer1/utils/config.py` defines the base config class:\n",
    "\n",
    "```python\n",
    "class Config:\n",
    "    model: LazyDict\n",
    "    optimizer: LazyDict\n",
    "    scheduler: LazyDict\n",
    "    dataloader_train: LazyDict\n",
    "    dataloader_val: LazyDict\n",
    "    job: JobConfig\n",
    "    trainer: TrainerConfig\n",
    "    checkpoint: CheckpointConfig\n",
    "```\n",
    "\n",
    "`cosmos_transfer1/diffusion/config/config_train.py` actually sets the default values for the base class:\n",
    "\n",
    "```python\n",
    "class Config(config.Config):\n",
    "    defaults: List[Any] = attrs.field(\n",
    "        factory=lambda: [\n",
    "            \"_self_\",\n",
    "            {\"data_train\": None},\n",
    "            {\"data_val\": None},\n",
    "            {\"optimizer\": \"fusedadamw\"},\n",
    "            {\"scheduler\": \"lambdalinear\"},\n",
    "            {\"callbacks\": None},\n",
    "            #\n",
    "            {\"net\": None},\n",
    "            {\"net_ctrl\": None},\n",
    "            {\"hint_key\": \"control_input_edge\"},\n",
    "            {\"conditioner\": \"ctrlnet_add_fps_image_size_padding_mask\"},\n",
    "            {\"pixel_corruptor\": None},\n",
    "            {\"fsdp\": None},\n",
    "            {\"ema\": \"power\"},\n",
    "            {\"checkpoint\": \"local\"},\n",
    "            {\"ckpt_klass\": \"multi_rank\"},\n",
    "            {\"tokenizer\": \"vae1\"},\n",
    "            # the list is with order, we need global experiment to be the last one\n",
    "            {\"experiment\": None},\n",
    "        ]\n",
    "    )\n",
    "    model_obj: LazyDict = L(VideoDiffusionModelWithCtrl)(\n",
    "        config=PLACEHOLDER,\n",
    "    )\n",
    "    checkpoint: CheckpointConfig = attrs.field(factory=CheckpointConfig)\n",
    "```\n",
    "</details>\n",
    "\n",
    "## Train.py\n",
    "\n",
    "## EMA\n",
    "\n",
    "## DiT Blocks\n",
    "\n",
    "## Control Blocks\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
